{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MIDIMultiOutputModel.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXoz9OWQ9Hg5"
      },
      "source": [
        "# IMPORT SECTION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmi2w3wn81Nt"
      },
      "source": [
        "!pip install --upgrade music21==6.7.1\n",
        "from music21 import converter, instrument, note, chord\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Flatten, Dropout\n",
        "%load_ext tensorboard \n",
        "from tensorboard import notebook\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pU-b0Ir867r"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive', force_remount=False)\n",
        "\n",
        "MIDI_PATH=\"/content/drive/My Drive/MLFolder/Onlab/MIDI_Iron_Maiden/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xD3TiCZW88Ne"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qBP01LM9L16"
      },
      "source": [
        "# UTILITY FUNCTIONS SECTION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5b81mgL89hI"
      },
      "source": [
        "def most_frequent(paramlist): \n",
        "    #https://www.geeksforgeeks.org/python-find-most-frequent-element-in-a-list/\n",
        "    counter = 0\n",
        "    num = paramlist[0] \n",
        "      \n",
        "    for i in paramlist: \n",
        "        curr_frequency = paramlist.count(i) \n",
        "        if(curr_frequency> counter): \n",
        "            counter = curr_frequency \n",
        "            num = i \n",
        "  \n",
        "    return num \n",
        "\n",
        "def get_notes_from_chord(chord):\n",
        "    if chord.startswith(\"<music21.chord.Chord \"):\n",
        "        chord = chord[len(\"<music21.chord.Chord \"):]\n",
        "    if chord.endswith(\">\"):\n",
        "        chord = chord[:-1]\n",
        "    chord = chord.replace(\" \", \",\")\n",
        "    return chord\n",
        "\n",
        "def get_number_from_duration(duration):\n",
        "    if duration.startswith(\"<music21.duration.Duration \"):\n",
        "        duration = duration[len(\"<music21.duration.Duration \"):]\n",
        "    if duration.endswith(\">\"):\n",
        "        duration = duration[:-1]\n",
        "    duration = duration.replace(\" \", \",\")\n",
        "    return duration\n",
        "\n",
        "def create_mapper(chords):\n",
        "    pitchnames = sorted(set(str(item) for item in chords))\n",
        "    mapper = dict((note, number) for number, note in enumerate(pitchnames))\n",
        "\n",
        "    return mapper\n",
        "\n",
        "def encode_using_mapper(chords, mapper):\n",
        "    encodedsong=[]\n",
        "    for c in chords:\n",
        "        encodedsong.append(mapper[str(c)])\n",
        "\n",
        "    return encodedsong\n",
        "\n",
        "\n",
        "def decode_chords_using_mapper(numbers, mapper):\n",
        "    outputnotes = []\n",
        "    for number in numbers:\n",
        "        outputnotes.append(chord_from_string(get_notes_from_chord(get_key_from_value(number, mapper))))\n",
        "\n",
        "    return outputnotes\n",
        "\n",
        "def combine_chords_with_durations(chords, durations):\n",
        "    combined = []\n",
        "\n",
        "    for i, j in zip(chords, durations):\n",
        "        i = get_notes_from_chord(str(i))\n",
        "        j = get_number_from_duration(str(j))\n",
        "        combined.append(i + ';' + j)\n",
        "\n",
        "    return combined\n",
        "\n",
        "def make_slices(data, slice_length):\n",
        "    for song in tqdm(data):\n",
        "        if len(song) > slice_length:\n",
        "\n",
        "            input = []\n",
        "            output = []\n",
        "            slice = []\n",
        "\n",
        "            for idx, number in enumerate(song):\n",
        "                if idx < slice_length:\n",
        "                    slice.append(number)\n",
        "\n",
        "            input.append(slice.copy())\n",
        "            output.append(song[slice_length])\n",
        "\n",
        "            # Sliding window\n",
        "            for idx, number in enumerate(song):\n",
        "                if idx >= slice_length and (idx + 1) < len(song):\n",
        "                    slice.pop(0)\n",
        "                    slice.append(number)\n",
        "                    input.append(slice.copy())  # Copy is necessary, because of how pointers and lists work in Python\n",
        "                    output.append(song[idx + 1])\n",
        "\n",
        "    return input, output\n",
        "\n",
        "def parse_everything_together(data, slice_length):\n",
        "\n",
        "    notes=[]\n",
        "    input=[]\n",
        "    output=[]\n",
        "    slice = []\n",
        "\n",
        "    for song in tqdm(data):\n",
        "        for number in song:\n",
        "            notes.append(number)\n",
        "    \n",
        "    for idx, note in tqdm(enumerate(notes)):\n",
        "        if idx < slice_length:\n",
        "            slice.append(number)\n",
        "\n",
        "    input.append(slice.copy())\n",
        "    output.append(notes[slice_length])\n",
        "\n",
        "    # Sliding window\n",
        "    for idx, number in tqdm(enumerate(notes)):\n",
        "        if idx >= slice_length and (idx + 1) < len(notes):\n",
        "            slice.pop(0)\n",
        "            slice.append(number)\n",
        "            input.append(slice.copy())  # Copy is necessary, because of how pointers and lists work in Python\n",
        "            output.append(notes[idx + 1])\n",
        "        \n",
        "    return input, output\n",
        "\n",
        "def get_key_from_value(value, dict):\n",
        "    return list(dict.keys())[list(dict.values()).index(value)]\n",
        "\n",
        "def get_notes_from_chord(chord):\n",
        "    if chord.startswith(\"<music21.chord.Chord \"):\n",
        "        chord = chord[len(\"<music21.chord.Chord \"):]\n",
        "    if chord.endswith(\">\"):\n",
        "        chord = chord[:-1]\n",
        "    chord = chord.replace(\" \", \",\")\n",
        "    return chord\n",
        "\n",
        "def get_number_from_duration(duration):\n",
        "    if duration.startswith(\"<music21.duration.Duration \"):\n",
        "        duration = duration[len(\"<music21.duration.Duration \"):]\n",
        "    if duration.endswith(\">\"):\n",
        "        duration = duration[:-1]\n",
        "    duration = duration.replace(\" \", \",\")\n",
        "    return duration\n",
        "\n",
        "def chord_from_string(chordstring):\n",
        "    notes = chordstring.split(\";\")\n",
        "    return chord.Chord(notes)\n",
        "\n",
        "\n",
        "def convert_to_float(frac_str):\n",
        "    #From: https://stackoverflow.com/questions/1806278/convert-fraction-to-float\n",
        "    try:\n",
        "        return float(frac_str)\n",
        "    except ValueError:\n",
        "        num, denom = frac_str.split('/')\n",
        "        try:\n",
        "            leading, num = num.split(' ')\n",
        "            whole = float(leading)\n",
        "        except ValueError:\n",
        "            whole = 0\n",
        "        frac = float(num) / float(denom)\n",
        "        return whole - frac if whole < 0 else whole + frac\n",
        "\n",
        "#Source: https://github.com/alexissa32/DataScienceMusic\n",
        "def create_midi_without_chords(prediction_output, target_instrument = instrument.Piano(), filename = 'test_output.mid'):\n",
        "    '''\n",
        "    First step:\n",
        "    Only notes, no chords\n",
        "    Static 4/4 beat\n",
        "    Rests\n",
        "    '''\n",
        "    offset = 0\n",
        "    output_notes = []\n",
        "\n",
        "    # create note and chord objects based on the values generated by the model\n",
        "    for pattern in prediction_output:\n",
        "        # pattern is a rest\n",
        "        if('rest' in pattern):\n",
        "            new_rest = note.Rest(pattern)\n",
        "            new_rest.offset = offset\n",
        "            new_rest.storedInstrument = target_instrument #???\n",
        "            output_notes.append(new_rest)\n",
        "        # pattern is a note\n",
        "        else:\n",
        "            new_note = note.Note(pattern)\n",
        "            new_note.offset = offset\n",
        "            new_note.storedInstrument = target_instrument\n",
        "            output_notes.append(new_note)\n",
        "        # increase offset each iteration so that notes do not stack\n",
        "        offset += 0.5\n",
        "\n",
        "    midi_stream = stream.Stream(output_notes)\n",
        "\n",
        "    midi_stream.write('midi', fp=filename)\n",
        "\n",
        "def create_midi_without_durations(prediction_output, target_instrument = instrument.Piano(), filename = 'test_output.mid'):\n",
        "    '''\n",
        "    Second step:\n",
        "    Chords and notes\n",
        "    Static 4/4 beat\n",
        "    Rests\n",
        "    '''\n",
        "    offset = 0\n",
        "    output_notes = []\n",
        "\n",
        "    # create note and chord objects based on the values generated by the model\n",
        "    for pattern in prediction_output:\n",
        "        # pattern is a chord\n",
        "        if ('chord' in pattern):\n",
        "            notes = []\n",
        "            pattern = get_notes_from_chord(pattern)\n",
        "            patternpitches = pattern.split(',')\n",
        "            for current_note in patternpitches:\n",
        "                new_note = note.Note(current_note)\n",
        "                new_note.storedInstrument = target_instrument\n",
        "                notes.append(new_note)\n",
        "            new_chord = chord.Chord(notes)\n",
        "            new_chord.offset = offset\n",
        "            output_notes.append(new_chord)\n",
        "        # pattern is a rest\n",
        "        elif('rest' in pattern):\n",
        "            new_rest = note.Rest(pattern)\n",
        "            new_rest.offset = offset\n",
        "            new_rest.storedInstrument = target_instrument #???\n",
        "            output_notes.append(new_rest)\n",
        "        # pattern is a note\n",
        "        else:\n",
        "            new_note = note.Note(pattern)\n",
        "            new_note.offset = offset\n",
        "            new_note.storedInstrument = target_instrument\n",
        "            output_notes.append(new_note)\n",
        "        # increase offset each iteration so that notes do not stack\n",
        "        offset += 0.5\n",
        "\n",
        "    midi_stream = stream.Stream(output_notes)\n",
        "\n",
        "    midi_stream.write('midi', fp=filename)\n",
        "\n",
        "def create_midi_with_durations(prediction_output, output_durations, target_instrument = instrument.Piano(), filename = 'test_output.mid'):\n",
        "    offset = 0\n",
        "    output_notes = []\n",
        "\n",
        "    # create note and chord objects based on the values generated by the model\n",
        "    for i in range(len(prediction_output)):\n",
        "        pattern = prediction_output[i]\n",
        "        duration = get_number_from_duration(output_durations[i])\n",
        "        # pattern is a chord\n",
        "        if ('chord' in pattern):\n",
        "            notes = []\n",
        "            pattern = get_notes_from_chord(pattern)\n",
        "            patternpitches = pattern.split(',')\n",
        "            for current_note in patternpitches:\n",
        "                new_note = note.Note(current_note)\n",
        "                new_note.storedInstrument = target_instrument\n",
        "                notes.append(new_note)\n",
        "            new_chord = chord.Chord(notes)\n",
        "            new_chord.offset = offset\n",
        "            output_notes.append(new_chord)\n",
        "        # pattern is a rest\n",
        "        elif('rest' in pattern):\n",
        "            new_rest = note.Rest(pattern)\n",
        "            new_rest.offset = offset\n",
        "            new_rest.storedInstrument = target_instrument #???\n",
        "            output_notes.append(new_rest)\n",
        "        # pattern is a note\n",
        "        else:\n",
        "            new_note = note.Note(pattern)\n",
        "            new_note.offset = offset\n",
        "            new_note.storedInstrument = target_instrument\n",
        "            output_notes.append(new_note)\n",
        "        # increase offset each iteration so that notes do not stack\n",
        "        offset += convert_to_float(duration)\n",
        "\n",
        "    midi_stream = stream.Stream(output_notes)\n",
        "\n",
        "    midi_stream.write('midi', fp=filename)\n",
        "\n",
        "def create_midi_with_embedded_durations(prediction_output, target_instrument = instrument.Piano(), filename = 'test_output.mid'):\n",
        "    offset = 0\n",
        "    output_notes = []\n",
        "\n",
        "    # create note and chord objects based on the values generated by the model\n",
        "    for i in range(len(prediction_output)):\n",
        "        pattern = prediction_output[i]\n",
        "        splitpattern = pattern.split(\";\")\n",
        "        pattern = splitpattern[0]\n",
        "\n",
        "        duration = get_number_from_duration(splitpattern[1])\n",
        "        # pattern is a rest\n",
        "        if('rest' in pattern):\n",
        "            new_rest = note.Rest(pattern)\n",
        "            new_rest.offset = offset\n",
        "            new_rest.storedInstrument = target_instrument #???\n",
        "            output_notes.append(new_rest)\n",
        "        # pattern is a chord\n",
        "        elif (',' in pattern):\n",
        "            notes = []\n",
        "            pattern = get_notes_from_chord(pattern)\n",
        "            patternpitches = pattern.split(',')\n",
        "            for current_note in patternpitches:\n",
        "                new_note = note.Note(current_note)\n",
        "                new_note.storedInstrument = target_instrument\n",
        "                notes.append(new_note)\n",
        "            new_chord = chord.Chord(notes)\n",
        "            new_chord.offset = offset\n",
        "            output_notes.append(new_chord)\n",
        "        # pattern is a note\n",
        "        else:\n",
        "            new_note = note.Note(pattern)\n",
        "            new_note.offset = offset\n",
        "            new_note.storedInstrument = target_instrument\n",
        "            output_notes.append(new_note)\n",
        "        # increase offset each iteration so that notes do not stack\n",
        "        offset += convert_to_float(duration)\n",
        "\n",
        "    midi_stream = stream.Stream(output_notes)\n",
        "\n",
        "    midi_stream.write('midi', fp=filename)\n",
        "\n",
        "def parse_midi_notes_and_durations():\n",
        "    midiparts = []\n",
        "\n",
        "    for file in tqdm(os.listdir(path)):\n",
        "        midi = converter.parse(os.path.join(path, file))\n",
        "\n",
        "        for part in midi.parts:\n",
        "            chords=[]\n",
        "            durations=[]\n",
        "            for element in part.notesAndRests:\n",
        "                if isinstance(element, note.Note):\n",
        "                    chords.append(chord.Chord([element]))\n",
        "                    durations.append(element.duration)\n",
        "                elif isinstance(element, chord.Chord):\n",
        "                    chords.append(element)\n",
        "                    durations.append(element.duration)\n",
        "                elif isinstance(element, note.Rest):\n",
        "                    chords.append(element)\n",
        "                    durations.append(element.duration)\n",
        "\n",
        "            if len(chords) > 0:\n",
        "                midiparts.append(MidiPart(file, part.partName, chords, durations))\n",
        "            else:\n",
        "                for voice in part.voices:\n",
        "                    chords=[]\n",
        "                    durations=[]\n",
        "                    for element in voice.notesAndRests:\n",
        "                        if isinstance(element, note.Note):\n",
        "                            chords.append(chord.Chord([element]))\n",
        "                            durations.append(element.duration)\n",
        "                        elif isinstance(element, chord.Chord):\n",
        "                            chords.append(element)\n",
        "                            durations.append(element.duration)\n",
        "                        elif isinstance(element, note.Rest):\n",
        "                            chords.append(element)\n",
        "                            durations.append(element.duration)\n",
        "\n",
        "                    midiparts.append(MidiPart(file, part.partName, chords, durations))\n",
        "\n",
        "    return midiparts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8pnr8le9S18"
      },
      "source": [
        "# PREPROCESSING SECTION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbSjTQ4t9R_z"
      },
      "source": [
        "class MidiPart:\n",
        "    def __init__(self, song, instrument, chords, durations):\n",
        "        self.song = song\n",
        "        self.instrument = instrument\n",
        "        self.chords = chords\n",
        "        self.durations = durations\n",
        "\n",
        "\n",
        "path = MIDI_PATH\n",
        "\n",
        "IRON_MAIDEN_INSTRUMENTS = ['Acoustic Guitar', 'Viola', 'Electric Bass', 'Brass', 'Sampler', 'Electric Guitar', 'Piano', 'StringInstrument']\n",
        "\n",
        "SLICE_LEN = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fz73sLZw9Yl6"
      },
      "source": [
        "midiparts = parse_midi_notes_and_durations()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kz_za8PG9ZyZ"
      },
      "source": [
        "allchords = []\n",
        "alldurations = []\n",
        "\n",
        "TARGET_INSTRUMENT = 'Electric Guitar'\n",
        "for i in midiparts:\n",
        "    if i.instrument == TARGET_INSTRUMENT:\n",
        "        if len(i.chords)>0 :\n",
        "            allchords.append(i.chords)\n",
        "            alldurations.append(i.durations)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8oG8EWi9g6I"
      },
      "source": [
        "#I create the Note/Chord/Rest to Number mapper here.\n",
        "\n",
        "mapperdata = []\n",
        "durationmapperdata = []\n",
        "\n",
        "for i in allchords:\n",
        "    for j in i:\n",
        "        mapperdata.append(j)\n",
        "\n",
        "for i in alldurations:\n",
        "    for j in i:\n",
        "        durationmapperdata.append(j)\n",
        "\n",
        "mapper = create_mapper(mapperdata)\n",
        "durationmapper = create_mapper(durationmapperdata)\n",
        "\n",
        "#I ensure that the dimensions are right\n",
        "assert(len(allchords)==len(alldurations))\n",
        "for i in range(len(allchords)):\n",
        "    assert(len(allchords[i])==len(alldurations[i]))\n",
        "\n",
        "\n",
        "\n",
        "#I encode the data here using the mapper.\n",
        "\n",
        "encoded_data = []\n",
        "encoded_durations = []\n",
        "\n",
        "for i in range(len(allchords)):\n",
        "    encoded = encode_using_mapper(allchords[i], mapper)\n",
        "    encoded_data.append(encoded)\n",
        "    encoded_dur = encode_using_mapper(alldurations[i], durationmapper)\n",
        "    encoded_durations.append(encoded_dur)\n",
        "\n",
        "assert(len(encoded_data)==len(encoded_durations))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8F-k9tY9jSN"
      },
      "source": [
        "#I filter out those chord sequences, which are majorly rests. I do this because I don't want the network to make a song, which contains nothing, but rests. Our dataset has about 100 guitar tracks, which contain rests for the majority of the track, and a guitar solo near the end.\n",
        "#An upgrade for this could be that I don't just delete those tracks from the dataset, but only filter out the rests, and leave the guitar solo in.\n",
        "\n",
        "\n",
        "restkeysvalues = []\n",
        "for j in mapper.keys():\n",
        "    if ( 'rest' in j):\n",
        "        restkeysvalues.append(mapper[j])\n",
        "\n",
        "cleared_encoded_data=[]\n",
        "cleared_encoded_durations=[]\n",
        "\n",
        "\n",
        "for i in range(len(encoded_data)):\n",
        "    if most_frequent(encoded_data[i]) not in restkeysvalues:\n",
        "        cleared_encoded_data.append(encoded_data[i])\n",
        "        cleared_encoded_durations.append(encoded_durations[i])\n",
        "    else:\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOz0gWM29qxq"
      },
      "source": [
        "note_input, note_output = parse_everything_together(cleared_encoded_data, SLICE_LEN)\n",
        "dur_input, dur_output = parse_everything_together(cleared_encoded_durations, SLICE_LEN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_tEyjrG9sg9"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "note_outputcnt = Counter(note_output)\n",
        "plt.bar(note_outputcnt.keys(), note_outputcnt.values())\n",
        "plt.show()\n",
        "\n",
        "function_like_array=[]\n",
        "for val in note_outputcnt.values():\n",
        "    function_like_array.append(val)\n",
        "\n",
        "function_like_array.sort()\n",
        "plt.plot(function_like_array)\n",
        "plt.show()\n",
        "#Plot it on log scale as well\n",
        "plt.yscale('log')\n",
        "plt.plot(function_like_array)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkC1KXwP9w6X"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "dur_outputcnt = Counter(dur_output)\n",
        "plt.bar(dur_outputcnt.keys(), dur_outputcnt.values())\n",
        "plt.show()\n",
        "\n",
        "function_like_array=[]\n",
        "for val in dur_outputcnt.values():\n",
        "    function_like_array.append(val)\n",
        "\n",
        "function_like_array.sort()\n",
        "plt.plot(function_like_array)\n",
        "plt.show()\n",
        "#Plot it on log scale as well\n",
        "plt.yscale('log')\n",
        "plt.plot(function_like_array)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeX8Cmvu90iM"
      },
      "source": [
        "note_outliers = []\n",
        "dur_outliers = []\n",
        "\n",
        "for i in note_outputcnt.keys():\n",
        "    if note_outputcnt[i] < 5:\n",
        "        note_outliers.append(i)\n",
        "\n",
        "for i in dur_outputcnt.keys():\n",
        "    if dur_outputcnt[i] < 5:\n",
        "        dur_outliers.append(i)\n",
        "\n",
        "print(len(note_outliers))\n",
        "print(len(dur_outliers))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Imh3TGiG-FDS"
      },
      "source": [
        "#However, this outlier filtering made things complicated. Now I have to make a new mapper, so that i won't end up with output classes containing 0 elements.\n",
        "\n",
        "note_mapper_list = [] #Idx of the mapper list is the new value, the element is the old value.\n",
        "dur_mapper_list = []\n",
        "new_note_output_elements = set(note_output)\n",
        "new_dur_output_elements = set(dur_output)\n",
        "\n",
        "for i in new_note_output_elements:\n",
        "    note_mapper_list.append(i)\n",
        "\n",
        "for i in new_dur_output_elements:\n",
        "    dur_mapper_list.append(i)\n",
        "\n",
        "new_note_output = []\n",
        "new_dur_output = []\n",
        "\n",
        "for i in note_output:\n",
        "    new_note_output.append(note_mapper_list.index(i))\n",
        "\n",
        "for i in dur_output:\n",
        "    new_dur_output.append(dur_mapper_list.index(i))\n",
        "\n",
        "note_output = new_note_output\n",
        "dur_output = new_dur_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkM5AcTu-Lcs"
      },
      "source": [
        "#Reshaping the input data to be compatible with LSTMs and normalizing it in the hope of better learning.\n",
        "\n",
        "note_input = np.reshape(np.asarray(note_input), (len(note_input), SLICE_LEN, 1))\n",
        "note_output = to_categorical(note_output)\n",
        "\n",
        "note_input=np.asarray(note_input) / float(len(mapper))\n",
        "note_output=np.asarray(note_output)\n",
        "\n",
        "\n",
        "dur_input = np.reshape(np.asarray(dur_input), (len(dur_input), SLICE_LEN, 1))\n",
        "dur_output = to_categorical(dur_output)\n",
        "\n",
        "dur_input=np.asarray(dur_input) / float(len(durationmapper))\n",
        "dur_output=np.asarray(dur_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mn3hrIkI-MBp"
      },
      "source": [
        "#TRAIN_VAL_TEST_SPLIT\n",
        "\n",
        "SEED = 54\n",
        "np.random.seed(SEED)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "note_X_train, note_X_test, note_Y_train, note_Y_test = train_test_split(note_input, note_output, test_size=0.01, random_state=SEED)\n",
        "note_X_train, note_X_val, note_Y_train, note_Y_val = train_test_split(note_X_train, note_Y_train, test_size=0.2, random_state=SEED)\n",
        "\n",
        "duration_X_train, duration_X_test, duration_Y_train, duration_Y_test = train_test_split(dur_input, dur_output, test_size=0.01, random_state=SEED)\n",
        "duration_X_train, duration_X_val, duration_Y_train, duration_Y_val = train_test_split(duration_X_train, duration_Y_train, test_size=0.2, random_state=SEED)\n",
        "\n",
        "startidx = np.random.randint(0, len(note_X_test)-1)\n",
        "\n",
        "starting_slice_notes = note_X_test[startidx]\n",
        "starting_slice_durations = duration_X_test[startidx]\n",
        "\n",
        "print(note_X_train.shape, note_X_val.shape, note_X_test.shape, note_Y_train.shape, note_Y_val.shape, note_Y_test.shape)\n",
        "print(duration_X_train.shape, duration_X_val.shape, duration_X_test.shape, duration_Y_train.shape, duration_Y_val.shape, duration_Y_test.shape)\n",
        "print(startidx)\n",
        "print(starting_slice_notes)\n",
        "print(starting_slice_durations)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o53yuo-MMmJP"
      },
      "source": [
        "# MODEL TRAINING SECTION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5O67Y-AMolF"
      },
      "source": [
        "MODEL_NAME = \"multioutputguitarmodel1\"\n",
        "\n",
        "MODEL_SAVE_PATH = \"/content/drive/MyDrive/MLFolder/Onlab/modelsaves\"\n",
        "\n",
        "TBPATH = \"/content/tblogs/\"+MODEL_NAME"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzAjR1YJMuSB"
      },
      "source": [
        "#Before creating the neural network, I define some important callbacks\n",
        "\n",
        "tb = TensorBoard(log_dir = TBPATH, write_images=True, histogram_freq=1)\n",
        "\n",
        "plateau = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.00005, verbose=1)\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', patience = 5, restore_best_weights = True, verbose=1)\n",
        "\n",
        "callbacks = [plateau, es, tb]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0K1BhRrMwrP"
      },
      "source": [
        "#I create the neural network model here.\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Concatenate, Multiply, Add, Dot, MultiHeadAttention\n",
        "\n",
        "notes_input = Input(shape=(note_X_train.shape[1], note_X_train.shape[2]), name=\"notes_in\")\n",
        "durations_input = Input(shape=(duration_X_train.shape[1], duration_X_train.shape[2]), name=\"durations_in\")\n",
        "\n",
        "x=LSTM(512, return_sequences=True)(notes_input)\n",
        "x=LSTM(256, return_sequences=True)(x)\n",
        "x=LSTM(32, return_sequences=True)(x)\n",
        "\n",
        "y=LSTM(64, return_sequences=True)(durations_input)\n",
        "y=LSTM(32, return_sequences=True)(y)\n",
        "\n",
        "notes_predictor = Dense(note_Y_train.shape[1], activation='softmax', name='notes_out')(x)\n",
        "durations_predictor = Dense(duration_Y_train.shape[1], activation='softmax', name='durations_out')(y)\n",
        "\n",
        "model = Model(\n",
        "    inputs=[notes_input, durations_input],\n",
        "    outputs=[notes_predictor, durations_predictor]\n",
        ")\n",
        "\n",
        "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'], loss_weights=[1.3, 0.7])\n",
        "\n",
        "model.fit({\"notes_in\" : note_X_train, \"durations_in\" : duration_X_train}, \n",
        "          {\"notes_out\" : note_Y_train, \"durations_out\" : duration_Y_train},\n",
        "          epochs=10000, batch_size=16,\n",
        "          validation_data=({\"notes_in\" : note_X_val, \"durations_in\" : duration_X_val},\n",
        "                           {\"notes_out\" : note_Y_val, \"durations_out\" : duration_Y_val}),\n",
        "           callbacks=callbacks)\n",
        "\n",
        "#model.evaluate(X_test, Y_test)\n",
        "\n",
        "model.save(MODEL_SAVE_PATH + MODEL_NAME + \".h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvCuX9QKNA2p"
      },
      "source": [
        "%tensorboard --logdir=\"/content/tblogs/multioutputguitarmodel1\" "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AaTL2BgNSZZ"
      },
      "source": [
        "# INFERENCE AND POSTPROCESSING SECTION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jAA7FsfNVjg"
      },
      "source": [
        "def sample(preds, temperature=1.0):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype(\"float64\")\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    preds = np.squeeze(preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcKZGc7RNXei"
      },
      "source": [
        "def generate_notes(model, starting_slice, starting_duration, mapper, mapperlist = None, temp=1.0, duration_temp=0.8):\n",
        "    \"\"\" Generate notes from the neural network based on a sequence of notes \"\"\"\n",
        "    pattern = starting_slice\n",
        "    duration_pattern = starting_duration\n",
        "    prediction_output_notes = []\n",
        "    prediction_output_durations = []\n",
        "    prediction_output = []\n",
        "\n",
        "    # generate 500 notes\n",
        "    for note_index in range(500):\n",
        "        prediction_input = np.reshape(pattern, (1, len(pattern), 1))\n",
        "        prediction_duration_input = np.reshape(duration_pattern, (1, len(duration_pattern), 1))\n",
        "\n",
        "        note_prediction, duration_prediction = model.predict({\"notes_in\" : prediction_input, \"durations_in\" : prediction_duration_input})\n",
        "\n",
        "        #prediction = sample(prediction, temp)\n",
        "        index = sample(note_prediction, temp)\n",
        "        duration_index = sample(duration_prediction, duration_temp)\n",
        "\n",
        "        #index = np.argmax(prediction)\n",
        "        if mapperlist is not None: #Idx of the mapper list is the new value, the element is the old value. This is used when I filter for outliers.\n",
        "            index=mapperlist[index]\n",
        "\n",
        "        result = get_key_from_value(index, mapper)\n",
        "        prediction_output_notes.append(result)\n",
        "\n",
        "        duration_result = get_key_from_value(duration_index, durationmapper)\n",
        "        prediction_output_durations.append(duration_result)\n",
        "\n",
        "        pattern = np.append(pattern, index/float(len(mapper)))\n",
        "        duration_pattern = np.append(duration_pattern, duration_index/float(len(durationmapper)))\n",
        "\n",
        "        pattern = pattern[1:len(pattern)]\n",
        "        duration_pattern = duration_pattern[1:len(duration_pattern)]\n",
        "\n",
        "    for i in range(len(prediction_output_notes)):\n",
        "        prediction_output.append(str(prediction_output_notes[i]) + ';' + str(prediction_output_durations[i]))\n",
        "\n",
        "    return prediction_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eh-Fbiv5NZt9"
      },
      "source": [
        "generated_outputs=[]\n",
        "\n",
        "temperatures = [0.5, 0.7, 0.8, 0.9, 1.0, 1.2]\n",
        "\n",
        "# DO NOT FORGET, IMPORTANT: When using outlier filtering, DO NOT FORGET to use the mapperlist parameter!!\n",
        "\n",
        "for temp in tqdm(temperatures):\n",
        "    generated_outputs.append(generate_notes(model, starting_slice_notes, starting_slice_durations, mapper, None, temp = temp, duration_temp=0.8))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cFA2l-uNljb"
      },
      "source": [
        "from music21 import stream\n",
        "\n",
        "STARTING_OFFSET = 0\n",
        "\n",
        "OUTPUT_PATH = \"/content/drive/MyDrive/MLFolder/Onlab/Outputs/\"\n",
        "\n",
        "for i, out in enumerate(generated_outputs):\n",
        "    create_midi_with_durations(out, target_instrument = instrument.ElectricGuitar(), filename = OUTPUT_PATH + MODEL_NAME + f'{i+STARTING_OFFSET}.mid')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}